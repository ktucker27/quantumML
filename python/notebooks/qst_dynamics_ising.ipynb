{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 6), \"Sonnet 2 requires Python >=3.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.3.1\n",
      "Sonnet version 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tree\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "  import sonnet.v2 as snt\n",
    "  tf.enable_v2_behavior()\n",
    "except ImportError:\n",
    "  import sonnet as snt\n",
    "\n",
    "print(\"TensorFlow version {}\".format(tf.__version__))\n",
    "print(\"Sonnet version {}\".format(snt.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/tuckerkj/git/quantumML/python/models/')\n",
    "import vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data = pd.read_table('/Users/tuckerkj/python/data/tfi_n10_upx/tfi_n10_upx_t0.txt', delimiter=' ', usecols=range(10)).values\n",
    "is_train = is_data[0:80000]\n",
    "is_test = is_data[80001:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = is_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data sliced for SGD\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(is_train)\n",
    "    .shuffle(1000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(-1))\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(is_test)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size)\n",
    "    .prefetch(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# 50K training steps unless otherwise noted\n",
    "# F does not appear to be impacted up to four digits by the sampling procedure\n",
    "# used to reconstruct the state\n",
    "\n",
    "# F = 0.9873 (larger training set) (no beta) (Loss: 4.582048416137695 Recon loss: 2.8563406467437744)\n",
    "intermediate_dim = [100]\n",
    "latent_dim = 4\n",
    "\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "enc = vae.Encoder(intermediate_dim, latent_dim)\n",
    "dec = vae.Decoder(intermediate_dim, original_dim)\n",
    "qstvae = vae.VAE(enc, dec)\n",
    "\n",
    "@tf.function\n",
    "def train_step(data, beta):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = qstvae(tf.cast(data, tf.float32))#, beta)\n",
    "    \n",
    "    trainable_variables = qstvae.trainable_variables\n",
    "    grads = tape.gradient(model_output['loss'], trainable_variables)\n",
    "    optimizer.apply(grads, trainable_variables)\n",
    "    \n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loss: 6.953280 recon loss: 6.880973\n",
      "2000 loss: 6.948987 recon loss: 6.890866\n",
      "3000 loss: 6.944779 recon loss: 6.897463\n",
      "4000 loss: 6.940837 recon loss: 6.902632\n",
      "5000 loss: 6.941781 recon loss: 6.908838\n",
      "6000 loss: 6.937864 recon loss: 6.910003\n",
      "7000 loss: 6.938973 recon loss: 6.911843\n",
      "8000 loss: 6.940070 recon loss: 6.915005\n",
      "9000 loss: 6.939018 recon loss: 6.915903\n",
      "10000 loss: 6.938797 recon loss: 6.913396\n",
      "11000 loss: 6.938328 recon loss: 6.916503\n",
      "12000 loss: 6.938834 recon loss: 6.917307\n",
      "13000 loss: 6.939315 recon loss: 6.917589\n",
      "14000 loss: 6.935915 recon loss: 6.915759\n",
      "15000 loss: 6.937923 recon loss: 6.920103\n",
      "16000 loss: 6.937616 recon loss: 6.918606\n",
      "17000 loss: 6.937326 recon loss: 6.921682\n",
      "18000 loss: 6.937371 recon loss: 6.920550\n",
      "19000 loss: 6.935030 recon loss: 6.919858\n",
      "20000 loss: 6.937127 recon loss: 6.921346\n",
      "21000 loss: 6.936165 recon loss: 6.921560\n",
      "22000 loss: 6.937075 recon loss: 6.923837\n",
      "23000 loss: 6.937419 recon loss: 6.923038\n",
      "24000 loss: 6.935955 recon loss: 6.922118\n",
      "25000 loss: 6.936821 recon loss: 6.922778\n",
      "26000 loss: 6.937217 recon loss: 6.921396\n",
      "27000 loss: 6.935869 recon loss: 6.924125\n",
      "28000 loss: 6.935399 recon loss: 6.923028\n",
      "29000 loss: 6.935662 recon loss: 6.922145\n",
      "30000 loss: 6.937505 recon loss: 6.925756\n",
      "31000 loss: 6.935510 recon loss: 6.925151\n",
      "32000 loss: 6.935430 recon loss: 6.926389\n",
      "33000 loss: 6.937103 recon loss: 6.926493\n",
      "34000 loss: 6.935479 recon loss: 6.924078\n",
      "35000 loss: 6.935623 recon loss: 6.926928\n",
      "36000 loss: 6.935363 recon loss: 6.924472\n",
      "37000 loss: 6.935448 recon loss: 6.924602\n",
      "38000 loss: 6.936109 recon loss: 6.925714\n",
      "39000 loss: 6.934054 recon loss: 6.927086\n",
      "40000 loss: 6.935808 recon loss: 6.926845\n",
      "41000 loss: 6.934462 recon loss: 6.923793\n",
      "42000 loss: 6.935249 recon loss: 6.925672\n",
      "43000 loss: 6.935545 recon loss: 6.925657\n",
      "44000 loss: 6.935027 recon loss: 6.926520\n",
      "45000 loss: 6.934113 recon loss: 6.927574\n",
      "46000 loss: 6.934315 recon loss: 6.924419\n",
      "47000 loss: 6.934598 recon loss: 6.924491\n",
      "48000 loss: 6.935580 recon loss: 6.926603\n",
      "49000 loss: 6.933957 recon loss: 6.925132\n",
      "50000 loss: 6.935178 recon loss: 6.927257\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_training_updates = 50000\n",
    "\n",
    "train_losses = []\n",
    "recon_losses = []\n",
    "for step_index, data in enumerate(train_dataset):\n",
    "    #beta = tf.constant(0.85*(step_index/float(num_training_updates)))\n",
    "    beta = 1.0\n",
    "    train_results = train_step(data, beta)\n",
    "    train_losses.append(train_results['loss'])\n",
    "    recon_losses.append(train_results['x_recon_loss'])\n",
    "    \n",
    "    if (step_index + 1) % 1000 == 0:\n",
    "        print('%d loss: %f recon loss: %f' % (step_index+1, np.mean(train_losses[-100:]), np.mean(recon_losses[-100:])))\n",
    "        \n",
    "    if (step_index + 1) % num_training_updates == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.9346699714660645 Recon loss: 6.927407741546631\n"
     ]
    }
   ],
   "source": [
    "# Look at validation set\n",
    "model_output = qstvae(tf.cast(is_test, dtype=tf.dtypes.float32))\n",
    "print('Loss: {} Recon loss: {}'.format(model_output['loss'], model_output['x_recon_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_array(a):\n",
    "    aa = []\n",
    "    for c in a:\n",
    "        if c == '0':\n",
    "            aa.append(0)\n",
    "        else:\n",
    "            aa.append(1)\n",
    "        \n",
    "    return np.array(aa)\n",
    "\n",
    "def bin_to_dec(b):\n",
    "    dec = 0\n",
    "    for idx, val in enumerate(b):\n",
    "        dec += val << (len(b) - idx - 1)\n",
    "        \n",
    "    return dec\n",
    "\n",
    "def update_counts2(psi, model, batch_size, per_batch_size):\n",
    "    # Sample from the prior distribution on the latent space and run through the decoder\n",
    "    latent_dim = model.decoder.hidden[0].input_size\n",
    "    z = tf.random.normal([batch_size, latent_dim], mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "    output = tf.nn.sigmoid(model.decoder(z))\n",
    "\n",
    "    # Sample from the binary distributions coming out of the decoder to get spins\n",
    "    all_meas_int = []\n",
    "    for idx in range(per_batch_size):\n",
    "        eps = tf.random.uniform(output.shape, minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "        all_meas_int.append(tf.cast(tf.math.greater_equal(output, eps), tf.int32).numpy())\n",
    "    meas_int = tf.concat(all_meas_int, axis=0).numpy()\n",
    "    \n",
    "    for ii in range(meas_int.shape[0]):\n",
    "        idx = bin_to_dec(meas_int[ii,:])\n",
    "        psi[idx] += 1\n",
    "\n",
    "def get_hist(n, meas_int):\n",
    "    psi = np.zeros(2**n)\n",
    "    for ii in range(meas_int.shape[0]):\n",
    "        idx = bin_to_dec(meas_int[ii,:])\n",
    "        psi[idx] += 1\n",
    "        \n",
    "    # Normalize\n",
    "    psi = np.sqrt(psi*(1.0/float(meas_int.shape[0])))\n",
    "    \n",
    "    return psi\n",
    "        \n",
    "def update_counts(psi, model, batch_size):\n",
    "    meas_int = model.sample(batch_size).numpy()\n",
    "    \n",
    "    for ii in range(meas_int.shape[0]):\n",
    "        idx = bin_to_dec(meas_int[ii,:])\n",
    "        psi[idx] += 1\n",
    "\n",
    "def get_psi(model, num_samples):\n",
    "    n = model.encoder.hidden[0].input_size\n",
    "    psi = np.zeros(2**n)\n",
    "    batch_size = 1000\n",
    "    per_batch_size = 10\n",
    "    total_samples = 0\n",
    "    while total_samples < num_samples:\n",
    "        update_counts(psi, model, batch_size)\n",
    "        #update_counts2(psi, model, batch_size, per_batch_size)\n",
    "        total_samples = total_samples + batch_size\n",
    "        \n",
    "        if total_samples % 100000 == 0:\n",
    "            print(total_samples)\n",
    "        \n",
    "    # Normalize\n",
    "    psi = np.sqrt(psi*(1.0/float(total_samples)))\n",
    "    #psi = np.sqrt(psi*(1.0/float(total_samples*per_batch_size)))\n",
    "    \n",
    "    return psi\n",
    "\n",
    "import math\n",
    "def get_psi_loss(model, num_samples):\n",
    "    n = model.encoder.hidden[0].input_size\n",
    "    norm = 0\n",
    "    psi = []\n",
    "    for d in range(2**n):\n",
    "        dbin = bit_array(np.binary_repr(d, width=n))\n",
    "        dbin_input = np.tile(dbin, (num_samples,1))\n",
    "        model_output = model(dbin_input.astype(float))\n",
    "        val = np.exp(-0.5*model_output['loss'])\n",
    "        psi.append(val)\n",
    "        norm = norm + val*val\n",
    "    norm = math.sqrt(norm)\n",
    "    \n",
    "    for ii in range(len(psi)):\n",
    "        psi[ii] = psi[ii]/norm\n",
    "        \n",
    "    return np.array(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "psi = get_psi(qstvae, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(psi, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0315119 , 0.03290897, 0.03168596, ..., 0.03098387, 0.02966479,\n",
       "       0.02993326])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the wave function\n",
    "np.savetxt('/Users/tuckerkj/python/data/vae_tfi_n10_upx_t0.txt', psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data = pd.read_table('/Users/tuckerkj/python/data/tfi_n10_upx/tfi_n10_upx_t1.txt', delimiter=' ', usecols=range(10)).values\n",
    "is_train = is_data[0:80000]\n",
    "is_test = is_data[80001:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = is_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data sliced for SGD\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(is_train)\n",
    "    .shuffle(1000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(-1))\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(is_test)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size)\n",
    "    .prefetch(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loss: 6.475722 recon loss: 6.450878\n",
      "2000 loss: 6.479484 recon loss: 6.441965\n",
      "3000 loss: 6.474957 recon loss: 6.438721\n",
      "4000 loss: 6.459610 recon loss: 6.412510\n",
      "5000 loss: 6.457504 recon loss: 6.403729\n",
      "6000 loss: 6.465859 recon loss: 6.384759\n",
      "7000 loss: 6.465143 recon loss: 6.343863\n",
      "8000 loss: 6.460498 recon loss: 6.311026\n",
      "9000 loss: 6.447575 recon loss: 6.271127\n",
      "10000 loss: 6.444769 recon loss: 6.240985\n",
      "11000 loss: 6.454764 recon loss: 6.241408\n",
      "12000 loss: 6.458661 recon loss: 6.223857\n",
      "13000 loss: 6.455796 recon loss: 6.210427\n",
      "14000 loss: 6.451104 recon loss: 6.202951\n",
      "15000 loss: 6.443942 recon loss: 6.185785\n",
      "16000 loss: 6.451401 recon loss: 6.180143\n",
      "17000 loss: 6.456146 recon loss: 6.177778\n",
      "18000 loss: 6.451734 recon loss: 6.167236\n",
      "19000 loss: 6.447408 recon loss: 6.167718\n",
      "20000 loss: 6.441145 recon loss: 6.155897\n",
      "21000 loss: 6.451897 recon loss: 6.157735\n",
      "22000 loss: 6.453855 recon loss: 6.152004\n",
      "23000 loss: 6.444924 recon loss: 6.132488\n",
      "24000 loss: 6.446603 recon loss: 6.137098\n",
      "25000 loss: 6.440881 recon loss: 6.126587\n",
      "26000 loss: 6.446468 recon loss: 6.126006\n",
      "27000 loss: 6.455286 recon loss: 6.119845\n",
      "28000 loss: 6.447094 recon loss: 6.111088\n",
      "29000 loss: 6.445147 recon loss: 6.107829\n",
      "30000 loss: 6.440571 recon loss: 6.104105\n",
      "31000 loss: 6.446797 recon loss: 6.107459\n",
      "32000 loss: 6.456990 recon loss: 6.111467\n",
      "33000 loss: 6.449343 recon loss: 6.101055\n",
      "34000 loss: 6.440858 recon loss: 6.095842\n",
      "35000 loss: 6.438884 recon loss: 6.094160\n",
      "36000 loss: 6.448329 recon loss: 6.110108\n",
      "37000 loss: 6.449986 recon loss: 6.083207\n",
      "38000 loss: 6.452832 recon loss: 6.107151\n",
      "39000 loss: 6.435859 recon loss: 6.094279\n",
      "40000 loss: 6.437771 recon loss: 6.078779\n",
      "41000 loss: 6.446821 recon loss: 6.100762\n",
      "42000 loss: 6.450787 recon loss: 6.094234\n",
      "43000 loss: 6.451638 recon loss: 6.102478\n",
      "44000 loss: 6.442377 recon loss: 6.103507\n",
      "45000 loss: 6.435865 recon loss: 6.090249\n",
      "46000 loss: 6.446918 recon loss: 6.099862\n",
      "47000 loss: 6.451308 recon loss: 6.095197\n",
      "48000 loss: 6.446208 recon loss: 6.093320\n",
      "49000 loss: 6.443963 recon loss: 6.096704\n",
      "50000 loss: 6.437124 recon loss: 6.087459\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_training_updates = 50000\n",
    "\n",
    "train_losses = []\n",
    "recon_losses = []\n",
    "for step_index, data in enumerate(train_dataset):\n",
    "    #beta = tf.constant(0.85*(step_index/float(num_training_updates)))\n",
    "    beta = 1.0\n",
    "    train_results = train_step(data, beta)\n",
    "    train_losses.append(train_results['loss'])\n",
    "    recon_losses.append(train_results['x_recon_loss'])\n",
    "    \n",
    "    if (step_index + 1) % 1000 == 0:\n",
    "        print('%d loss: %f recon loss: %f' % (step_index+1, np.mean(train_losses[-100:]), np.mean(recon_losses[-100:])))\n",
    "        \n",
    "    if (step_index + 1) % num_training_updates == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "psi = get_psi(qstvae, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the wave function\n",
    "np.savetxt('/Users/tuckerkj/python/data/vae_tfi_n10_upx_t1.txt', psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = get_hist(original_dim, is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the histogram\n",
    "np.savetxt('/Users/tuckerkj/python/data/hist_tfi_n10_upx_t1.txt', psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
