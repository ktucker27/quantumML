{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 6), \"Sonnet 2 requires Python >=3.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.3.1\n",
      "Sonnet version 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tree\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "  import sonnet.v2 as snt\n",
    "  tf.enable_v2_behavior()\n",
    "except ImportError:\n",
    "  import sonnet as snt\n",
    "\n",
    "print(\"TensorFlow version {}\".format(tf.__version__))\n",
    "print(\"Sonnet version {}\".format(snt.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tuckerkj/git/quantumML/python\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data = pd.read_table('/Users/tuckerkj/Google Drive/Research/QML/data/quc_examples/Tutorial1_TrainPosRealWaveFunction/tfim1d_data.txt', delimiter=' ', usecols=range(10)).values\n",
    "is_train = is_data[0:7999]\n",
    "is_test = is_data[8000:9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = is_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 100\n",
    "batch_size = 128\n",
    "latent_dim = 10\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(snt.Module):\n",
    "    def __init__(self, hdims, latent_dim, name=None):\n",
    "        super(Encoder, self).__init__(name)\n",
    "        \n",
    "        self.hidden = []\n",
    "        for idx, hdim in enumerate(hdims):\n",
    "            layer_name = 'hidden{}'.format(idx)\n",
    "            self.hidden.append(snt.Linear(hdim, name=layer_name))\n",
    "            \n",
    "        self.z_mean = snt.Linear(latent_dim, name='z_mean')\n",
    "        self.z_log_var = snt.Linear(latent_dim, name='z_log_var')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Run the input through the encoder network\n",
    "        for hidden_layer in self.hidden:\n",
    "            x = tf.nn.relu(hidden_layer(x))\n",
    "        \n",
    "        # Use the reparameterization trick to calculate the latent variables\n",
    "        out_mean = self.z_mean(x)\n",
    "        out_log_var = self.z_log_var(x)\n",
    "        \n",
    "        eps = tf.random.normal(out_mean.shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.double)\n",
    "        out_z = eps*tf.exp(0.5*out_log_var) + out_mean\n",
    "        \n",
    "        return {\n",
    "            'mean': out_mean,\n",
    "            'log_var': out_log_var,\n",
    "            'z': out_z\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder([intermediate_dim], latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = enc(is_train.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7999, 4), dtype=float64, numpy=\n",
       "array([[-0.84969135, -0.14474033,  0.94266648, -1.33234972],\n",
       "       [ 1.59825231,  1.27660613, -0.59592931,  0.44321434],\n",
       "       [-1.16510074,  0.44970454,  0.49334116,  2.55032563],\n",
       "       ...,\n",
       "       [-0.70828427,  0.56456185, -0.09377427, -0.41238832],\n",
       "       [ 1.07908452,  0.89689959,  0.55525758, -0.81118676],\n",
       "       [ 0.80897817, -0.79185813, -0.88526631, -1.26299141]])>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7999, 4])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['z'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(snt.Module):\n",
    "    def __init__(self, hdims, vdim, apply_sigmoid=False, name=None):\n",
    "        super(Decoder, self).__init__(name)\n",
    "        \n",
    "        self.apply_sigmoid = apply_sigmoid\n",
    "        \n",
    "        self.hidden = []\n",
    "        for idx, hdim in enumerate(hdims):\n",
    "            layer_name = 'hidden{}'.format(idx)\n",
    "            self.hidden.append(snt.Linear(hdim, name=layer_name))\n",
    "            \n",
    "        self.visible = snt.Linear(vdim, 'visible')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        for hidden_layer in self.hidden:\n",
    "            x = tf.nn.relu(hidden_layer(x))\n",
    "        \n",
    "        if self.apply_sigmoid:\n",
    "            output = tf.nn.sigmoid(self.visible(x))\n",
    "        else:\n",
    "            output = self.visible(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.cast(tf.math.log(2. * np.pi), tf.float64)\n",
    "\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** tf.cast(2., tf.float64) * tf.cast(tf.exp(-logvar), tf.float64) + logvar + log2pi),\n",
    "        axis=raxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(snt.Module):\n",
    "    def __init__(self, encoder, decoder, name=None):\n",
    "        super(VAE, self).__init__(name)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        # Run the encoder and decoder\n",
    "        encoder_output = self.encoder(x)\n",
    "        x_recon = self.decoder(encoder_output['z'])\n",
    "        \n",
    "        # Compute the loss function\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_recon, labels=x)\n",
    "        log_pxz = -tf.reduce_sum(cross_ent, axis=1)\n",
    "        log_pz = log_normal_pdf(encoder_output['z'], tf.cast(0.0, tf.float64), tf.cast(0.0, tf.float64))\n",
    "        log_qzx = log_normal_pdf(encoder_output['z'], encoder_output['mean'], encoder_output['log_var'])\n",
    "        loss = -tf.reduce_mean(log_pxz + log_pz - log_qzx)\n",
    "        \n",
    "        return {\n",
    "            'x_recon': x_recon,\n",
    "            'loss': loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder([intermediate_dim], original_dim, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vae = dec(out_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7999, 10), dtype=float64, numpy=\n",
       "array([[0.49805634, 0.56821103, 0.50252593, ..., 0.48917527, 0.49358371,\n",
       "        0.50441736],\n",
       "       [0.53601288, 0.5935486 , 0.50996864, ..., 0.58233535, 0.53262848,\n",
       "        0.60569744],\n",
       "       [0.5478654 , 0.61765784, 0.51126612, ..., 0.57514374, 0.53254697,\n",
       "        0.60227551],\n",
       "       ...,\n",
       "       [0.52100069, 0.58701677, 0.50656475, ..., 0.51436673, 0.50555004,\n",
       "        0.52687339],\n",
       "       [0.48744812, 0.52148054, 0.50447984, ..., 0.4845681 , 0.50866468,\n",
       "        0.45236271],\n",
       "       [0.47266674, 0.5267072 , 0.49364781, ..., 0.48846628, 0.51004165,\n",
       "        0.43183574]])>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = vae(is_test.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_recon': <tf.Tensor: shape=(1999, 10), dtype=float64, numpy=\n",
       " array([[-2.41030074, -3.95024537, -4.11566051, ..., -2.13874472,\n",
       "         -0.05711204,  0.65082204],\n",
       "        [ 1.93131619,  3.65198849,  3.6826388 , ...,  5.53376829,\n",
       "          4.68197597,  2.42143408],\n",
       "        [-0.06304897, -1.015196  , -3.01258011, ..., -2.76105152,\n",
       "         -0.43423799,  0.81827818],\n",
       "        ...,\n",
       "        [-0.44084999, -0.44696913, -0.18770231, ...,  2.62859023,\n",
       "          1.88377851,  0.9240937 ],\n",
       "        [ 3.05381366,  3.74502294,  1.39615498, ...,  1.20114837,\n",
       "          0.36089653, -0.14511819],\n",
       "        [ 0.61598874,  1.84835129,  2.45726687, ...,  0.88366842,\n",
       "         -0.96806828, -1.01429355]])>,\n",
       " 'loss': <tf.Tensor: shape=(), dtype=float64, numpy=5.460200439432734>}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utility function to show progress bar.\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_meas = is_train.shape[0]\n",
    "\n",
    "def progress_bar(generator):\n",
    "  return tqdm(\n",
    "      generator,\n",
    "      unit='measurements',\n",
    "      unit_scale=batch_size,\n",
    "      total=(num_meas // batch_size) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "learning_rate = 3e-4\n",
    "input_shape = (original_dim, )\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# F = 0.9322\n",
    "#intermediate_dim = [500, 500]\n",
    "#latent_dim = 10\n",
    "\n",
    "# F = 0.9255\n",
    "intermediate_dim = [100]\n",
    "latent_dim = 4\n",
    "\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "enc = Encoder(intermediate_dim, latent_dim)\n",
    "dec = Decoder(intermediate_dim, original_dim, apply_sigmoid=False)\n",
    "vae = VAE(enc, dec)\n",
    "\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = vae(tf.cast(data, tf.float64))\n",
    "    \n",
    "    trainable_variables = vae.trainable_variables\n",
    "    grads = tape.gradient(model_output['loss'], trainable_variables)\n",
    "    optimizer.apply(grads, trainable_variables)\n",
    "    \n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data sliced for SGD\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(is_train)\n",
    "    .shuffle(1000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(-1))\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(is_test)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size)\n",
    "    .prefetch(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss: 7.072758\n",
      "200 loss: 6.799180\n",
      "300 loss: 6.474688\n",
      "400 loss: 6.158727\n",
      "500 loss: 6.000201\n",
      "600 loss: 5.908706\n",
      "700 loss: 5.803221\n",
      "800 loss: 5.731229\n",
      "900 loss: 5.668533\n",
      "1000 loss: 5.634540\n",
      "1100 loss: 5.607149\n",
      "1200 loss: 5.581994\n",
      "1300 loss: 5.556939\n",
      "1400 loss: 5.531736\n",
      "1500 loss: 5.526729\n",
      "1600 loss: 5.499567\n",
      "1700 loss: 5.487525\n",
      "1800 loss: 5.480589\n",
      "1900 loss: 5.467995\n",
      "2000 loss: 5.465597\n",
      "2100 loss: 5.459521\n",
      "2200 loss: 5.451526\n",
      "2300 loss: 5.450784\n",
      "2400 loss: 5.449907\n",
      "2500 loss: 5.447725\n",
      "2600 loss: 5.445880\n",
      "2700 loss: 5.427263\n",
      "2800 loss: 5.443211\n",
      "2900 loss: 5.432085\n",
      "3000 loss: 5.427277\n",
      "3100 loss: 5.434038\n",
      "3200 loss: 5.428937\n",
      "3300 loss: 5.435452\n",
      "3400 loss: 5.426408\n",
      "3500 loss: 5.425033\n",
      "3600 loss: 5.435596\n",
      "3700 loss: 5.418318\n",
      "3800 loss: 5.420051\n",
      "3900 loss: 5.422524\n",
      "4000 loss: 5.408796\n",
      "4100 loss: 5.417993\n",
      "4200 loss: 5.399564\n",
      "4300 loss: 5.428816\n",
      "4400 loss: 5.420942\n",
      "4500 loss: 5.409031\n",
      "4600 loss: 5.412012\n",
      "4700 loss: 5.410764\n",
      "4800 loss: 5.413084\n",
      "4900 loss: 5.406992\n",
      "5000 loss: 5.411330\n",
      "5100 loss: 5.405646\n",
      "5200 loss: 5.407812\n",
      "5300 loss: 5.400537\n",
      "5400 loss: 5.407161\n",
      "5500 loss: 5.405159\n",
      "5600 loss: 5.405609\n",
      "5700 loss: 5.399772\n",
      "5800 loss: 5.405049\n",
      "5900 loss: 5.405916\n",
      "6000 loss: 5.401105\n",
      "6100 loss: 5.403948\n",
      "6200 loss: 5.405931\n",
      "6300 loss: 5.398167\n",
      "6400 loss: 5.396259\n",
      "6500 loss: 5.399527\n",
      "6600 loss: 5.403347\n",
      "6700 loss: 5.398946\n",
      "6800 loss: 5.404523\n",
      "6900 loss: 5.405702\n",
      "7000 loss: 5.401429\n",
      "7100 loss: 5.402378\n",
      "7200 loss: 5.392155\n",
      "7300 loss: 5.393642\n",
      "7400 loss: 5.397323\n",
      "7500 loss: 5.400120\n",
      "7600 loss: 5.398008\n",
      "7700 loss: 5.388296\n",
      "7800 loss: 5.396138\n",
      "7900 loss: 5.398726\n",
      "8000 loss: 5.398129\n",
      "8100 loss: 5.403136\n",
      "8200 loss: 5.392632\n",
      "8300 loss: 5.410040\n",
      "8400 loss: 5.396945\n",
      "8500 loss: 5.385432\n",
      "8600 loss: 5.401829\n",
      "8700 loss: 5.392341\n",
      "8800 loss: 5.399373\n",
      "8900 loss: 5.387069\n",
      "9000 loss: 5.396998\n",
      "9100 loss: 5.397916\n",
      "9200 loss: 5.395026\n",
      "9300 loss: 5.392881\n",
      "9400 loss: 5.393054\n",
      "9500 loss: 5.389475\n",
      "9600 loss: 5.397461\n",
      "9700 loss: 5.394268\n",
      "9800 loss: 5.379802\n",
      "9900 loss: 5.398395\n",
      "10000 loss: 5.392455\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_training_updates = 10000\n",
    "\n",
    "train_losses = []\n",
    "for step_index, data in enumerate(train_dataset):\n",
    "    train_results = train_step(data)\n",
    "    train_losses.append(train_results['loss'])\n",
    "    \n",
    "    if (step_index + 1) % 100 == 0:\n",
    "        print('%d loss: %f' % (step_index+1, np.mean(train_losses[-100:])))\n",
    "        \n",
    "    if (step_index + 1) % num_training_updates == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_recon': <tf.Tensor: shape=(1999, 10), dtype=float64, numpy=\n",
      "array([[-1.30727181, -3.54919053, -4.69703729, ..., -5.44847633,\n",
      "        -4.98149683, -3.04263431],\n",
      "       [ 2.07257087,  2.37946334,  0.69731315, ...,  4.22436914,\n",
      "         3.33188312,  1.53229337],\n",
      "       [-0.63764162, -1.26515629, -1.87520167, ..., -3.63851825,\n",
      "        -2.23825483, -0.78866909],\n",
      "       ...,\n",
      "       [ 0.08981474,  0.32190265,  0.31280718, ...,  2.05090455,\n",
      "         2.46932876,  1.83831821],\n",
      "       [ 0.62596565,  1.30627108,  1.68272043, ...,  1.22994615,\n",
      "        -0.79836201, -1.4194339 ],\n",
      "       [ 0.94270487,  1.31685063,  1.33196763, ...,  0.66857458,\n",
      "        -1.80954938, -2.33935259]])>, 'loss': <tf.Tensor: shape=(), dtype=float64, numpy=5.493371116277098>}\n"
     ]
    }
   ],
   "source": [
    "# Look at validation set\n",
    "model_output = vae(is_test.astype(float))\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_array(a):\n",
    "    aa = []\n",
    "    for c in a:\n",
    "        if c == '0':\n",
    "            aa.append(0)\n",
    "        else:\n",
    "            aa.append(1)\n",
    "        \n",
    "    return np.array(aa)\n",
    "\n",
    "def bin_to_dec(b):\n",
    "    dec = 0\n",
    "    for idx, val in enumerate(b):\n",
    "        dec += val << (len(b) - idx - 1)\n",
    "        \n",
    "    return dec\n",
    "\n",
    "def update_counts(psi, vae, batch_size):\n",
    "    latent_dim = vae.decoder.hidden[0].input_size\n",
    "    z = tf.random.normal([batch_size, latent_dim], mean=0.0, stddev=1.0, dtype=tf.dtypes.double)\n",
    "    output = tf.nn.sigmoid(vae.decoder(z))\n",
    "    \n",
    "    #meas = tf.math.round(tf.nn.sigmoid(vae.decoder(z)))\n",
    "    #meas_int = meas.numpy().astype(int)\n",
    "    \n",
    "    eps = tf.random.uniform(output.shape, minval=0, maxval=1, dtype=tf.dtypes.float64)\n",
    "    meas_int = tf.cast(tf.math.greater_equal(output, eps), tf.int32).numpy()\n",
    "    \n",
    "    for ii in range(meas_int.shape[0]):\n",
    "        idx = bin_to_dec(meas_int[ii,:])\n",
    "        psi[idx] += 1\n",
    "\n",
    "def get_psi(vae, num_samples):\n",
    "    n = vae.encoder.hidden[0].input_size\n",
    "    psi = np.zeros(2**n)\n",
    "    batch_size = 1000\n",
    "    total_samples = 0\n",
    "    while total_samples < num_samples:\n",
    "        update_counts(psi, vae, batch_size)\n",
    "        total_samples = total_samples + batch_size\n",
    "        \n",
    "    # Normalize\n",
    "    psi = np.sqrt(psi*(1.0/float(total_samples)))\n",
    "    \n",
    "    return psi\n",
    "\n",
    "import math\n",
    "def get_psi_loss(vae, num_samples):\n",
    "    n = vae.encoder.hidden[0].input_size\n",
    "    norm = 0\n",
    "    psi = []\n",
    "    for d in range(2**n):\n",
    "        dbin = bit_array(np.binary_repr(d, width=n))\n",
    "        dbin_input = np.tile(dbin, (num_samples,1))\n",
    "        model_output = vae(dbin_input.astype(float))\n",
    "        val = np.exp(-0.5*model_output['loss'])\n",
    "        psi.append(val)\n",
    "        norm = norm + val*val\n",
    "    norm = math.sqrt(norm)\n",
    "    \n",
    "    for ii in range(len(psi)):\n",
    "        psi[ii] = psi[ii]/norm\n",
    "        \n",
    "    return np.array(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = get_psi(vae, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = get_psi_loss(vae, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(psi, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28247301, 0.1441527 , 0.09196739, ..., 0.09613012, 0.1405169 ,\n",
       "       0.28005892])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the wave function\n",
    "np.savetxt('/Users/tuckerkj/Google Drive/Research/QML/data/quc_examples/Tutorial1_TrainPosRealWaveFunction/ld_results/new_vae_psi_4d.txt', psi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n"
     ]
    }
   ],
   "source": [
    "a = ['a','b','c']\n",
    "for idx, val in enumerate(a):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = snt.Linear(intermediate_dim, name='test_lin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7999, 100), dtype=float64, numpy=\n",
       "array([[-0.46892147, -0.34728384, -0.74378709, ...,  0.29133378,\n",
       "        -0.26331127,  0.37414721],\n",
       "       [-0.04339453,  0.07576071, -1.16743727, ...,  0.93418193,\n",
       "         0.12609772,  0.85569389],\n",
       "       [ 0.02504136,  0.50216652, -1.25031824, ...,  0.6404713 ,\n",
       "         0.64565024,  0.74596843],\n",
       "       ...,\n",
       "       [ 0.15872905,  0.34844972, -1.76310892, ...,  0.21092538,\n",
       "         0.73613504,  0.02295605],\n",
       "       [ 0.31660287,  0.42498682, -0.7207022 , ..., -0.56906129,\n",
       "         0.97862829, -0.26926874],\n",
       "       [ 1.17475972,  0.32786622, -1.34069384, ...,  0.04867342,\n",
       "         0.99408893, -0.87915178]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin(is_train.astype(float))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
